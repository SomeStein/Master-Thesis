
\chapter{Introduction}
\label{ch:introduction} % fÃ¼r einen Textverweis auf das Kapitel: \ref{ch:introduction}

The \textit{Joint Spectral Radius} (JSR) was first introduced by G.-C. Rota and G. Strang in 1960 \citep{rotaNoteJointSpectral1960}. They described the JSR as the maximal exponential growth rate of a product of matrices taken from a finite set. Since its inception, the JSR has become a cornerstone in various mathematical and engineering disciplines due to its ability to encapsulate the asymptotic behavior of matrix long products. 

The concept gained significant traction in the 1990s when researchers began exploring its theoretical properties and practical implications. Notable advancements include its application in wavelet theory, where it assists in the construction of refinable functions \citep{daubechies1992sets} as well as in control theory, where it is used to analyze the stability of switched linear systems \citep{blondelSurveyComputationalComplexity2000}, for which we will give a small example in the following. The computational challenges associated with determining the JSR have inspired the development of several algorithms, such as the invariant-polytope method \citep{guglielmiExactComputationJoint2013} and the finite-tree method \citep{mollerTreebasedApproachJoint2014}.

Despite the progress, the JSR computation remains a challenging problem, particularly due to the exponential complexity of exploring all possible matrix products. This thesis seeks to contribute to this ongoing effort by leveraging the invariant-polytope algorithm and the finite-tree algorithm to create a hybrid methodology that mitigates their respective limitations.

\section{Motivation of the JSR}
[include linear switched dynamical system example]

\section*{Structure of the Thesis}
The remainder of this thesis is structured as follows: Chapter~\ref{ch:introduction} provides a sufficient background on the JSR and its basic properties. Chapters~\ref{ch:inv.poly} and~\ref{ch:finite-tree} present the ideas and concepts of the algorithms that will be exploited to form the proposed hybrid approach, outlining their theoretical foundation and algorithmic implementation. Chapter~\ref{ch:hybrid} discusses possible combinations of former approaches, proposes the so-called Tree-flavored-invariant-polytope algorithm, and brings proofs of termination which is the main result of this thesis. \emph{Chapter~\ref{ch:numerics}} presents numerical results on ... problems to analyze the efficiency and applicability of the hybrid algorithm. Chapter~\ref{ch:conclusion} concludes with insights and future directions.

\section{Theoretical background}

The \emph{joint spectral radius} (JSR) of a set of matrices is a generalization of the spectral radius for a single matrix. For a finite set of matrices $\mathcal{A} = \{A_1, A_2, \dots, A_m\}$, the JSR is defined as:
\begin{equation}
    \JSR(\mathcal{A}) = \lim_{k \to \infty} \max_{A_i \in \mathcal{A}} \|A_{i_k} \dots A_{i_1}\|^{1/k},
\end{equation}
where $\|\cdot\|$ denotes any submultiplicative matrix norm.

\begin{theorem}
    The JSR is well-defined and independent of the choice of the submultiplicative norm.
\end{theorem}

\begin{proof}{well-definedness\\}
Let $\| \cdot \|_1$   and $ \| \cdot \|_2 $ be two submultiplicative norms on $ \mathbb{R}^{n \times n} $. By equivalence of norms in finite-dimensional vector spaces, there exist constants $ c, C > 0 $ such that:
$$
c \|P\|_1 \leq \|P\|_2 \leq C \|P\|_1 \quad \forall P \in \mathbb{R}^{n \times n}
$$
Now if we consider this we get: 
$$
\lim_{k \to \infty} \max_{A_i \in \mathcal{A}} \|A_{i_k} \dots A_{i_1}\|_{1}^{1/k} 
$$
$$
\le \lim_{k \to \infty} {(\frac{1}{c})}^{\frac{1}{k}} \max_{A_i \in \mathcal{A}} \|A_{i_k} \dots A_{i_1}\|_{2}^{1/k} 
$$
$$
= \lim_{k \to \infty} \max_{A_i \in \mathcal{A}} \|A_{i_k} \dots A_{i_1}\|_{2}^{1/k} 
$$
$$
\le \lim_{k \to \infty} {C}^{\frac{1}{k}} \max_{A_i \in \mathcal{A}} \|A_{i_k} \dots A_{i_1}\|_{1}^{1/k}
$$
$$
= \lim_{k \to \infty} \max_{A_i \in \mathcal{A}} \|A_{i_k} \dots A_{i_1}\|_{1}^{1/k}
$$
Which concludes the proof.
\end{proof}

To understand the state-of-the-art algorithms considered as well as the main results that will follow, it is necessary to introduce some basic properties of the JSR.

\subsection*{Homogeneity}
\begin{proposition}
    For any set of matrices $\mathcal{A}$ and scalar $\alpha$, the JSR is homogeneous:
    \begin{equation}
        \JSR(\alpha \mathcal{A}) = |\alpha| \JSR(\mathcal{A})
    \end{equation}
\end{proposition}
\begin{proof}
    Let $\mathcal{A} = \{A_1, A_2, \dots, A_m\}$ and $\alpha \in \mathbb{R}$. Then:
    \begin{align*}
        \JSR(\alpha \mathcal{A}) &= \lim_{k \to \infty} \max_{A_i \in \mathcal{A}^k} \|\alpha A_{i_k} \cdot ... \cdot \alpha A_{i_1}\|^{1/k} \\
        &= |\alpha| \lim_{k \to \infty} \max_{A_i \in \mathcal{A}^k} \|A_{i_k} \cdot ... \cdot A_{i_1}\|^{1/k} = |\alpha| JSR(\mathcal{A})
    \end{align*}
\end{proof}

\subsection*{Irreducibility}
\begin{definition}
    A set of matrices is called \emph{(commonly) reducible} if there exists a nontrivial subspace of $\mathbb{R}^n$ that is invariant under all matrices in the set. This means there exists a change of basis that block-triangularizes all matrices in $\mathcal{A}$ at the same time. If $\mathcal{A}$ is not reducible it is called \emph{irreducible}. 
\end{definition}

\subsection*{Three-member inequality}
The \emph{three-member inequality} provides essential bounds for the JSR. 

\begin{proposition}
    For any submultiplicative matrix norm $\|\cdot\|$, the inequality
    \begin{equation}
    \max_{P \in \mathcal{A}^k} \rho(P)^{\frac{1}{k}} \leq JSR(\mathcal{A}) \leq \max_{P \in \mathcal{A}^k} \| P\|^{\frac{1}{k}},
    \label{eq:three-member}
    \end{equation}
    holds for every $k \in \mathbb{N}$.
\end{proposition}
\begin{proof}
    The left side of the inequality follows from the definition of the JSR, as $\rho(P^k) = \rho(P)^k$ for any matrix $P$. The right side follows from a special case of Fekete's lemma.
\end{proof}
This result forms a starting point for many computational approaches as the bounds are sharp in the sense that both sides converge to the JSR as $k\rightarrow \infty$ (left side in $\limsup$).

\subsection*{Minimum over norms}
The JSR can be equivalently defined as the minimum over all submultiplicative norms:
\begin{equation}
    \JSR(\mathcal{A}) = \min_{\|\cdot\|} \max_{A \in \mathcal{A}} \|A\|.
\end{equation}
\begin{proof}{blondel\\}

\end{proof}

\subsection*{Finiteness Property}
Certain matrix sets exhibit the \emph{finiteness property}, which states, that there exists a finite sequence of matrices $A_{i_1}, \dots, A_{i_k}$, such that:
\begin{equation}
    \JSR(\mathcal{A}) = \|A_{i_k} \cdots A_{i_1}\|^{1/k}.
\end{equation}
While this property does not hold universally, it is essential for algorithmic approaches and termination in most cases.

\subsection*{Complexity}

\subsection*{Candidates and Generators}
Approximating the JSR requires identifying candidate products or \emph{generators} of the matrix set that contribute most significantly to the asymptotic growth rate. These generators are often derived through optimization techniques and their identification is a key step in computational algorithms.

\subsection*{Similarity and reducibility}
For any invertable matrix $T$ and reducible set $\mathcal{A}$
\begin{equation}
   \JSR(\mathcal{A}) = \JSR(T^{-1}\mathcal{A}T) 
\end{equation}

holds. Per definition, there exists a change of basis such that all $A \in \mathcal{A}$ are block-triangularized: 
$$\exists T: T^{-1}A_{i}T = 
\begin{bmatrix}
B_i & C_i  \\
0 & D_i
\end{bmatrix}
$$
Now we can split the set into two sets of blocks $B_i$ and $D_i$ and the JSR can be computed as:
\begin{equation}
    \JSR(\mathcal{A}) = \max\{\JSR(\{B_i: i = 1, \cdots J \}), \JSR(\{D_i: i = 1, \cdots J \})\}
\end{equation}
The proof can be seen in \citep{jungersJointSpectralRadius2009}.
This can be applied iteratively until the sets of blocks are all irreducible.
The problem was split into similar problems of smaller dimension.
For the following considerations we can now assume $\mathcal{A}$ to be irreducible.

\section{Preprocessing}
\label{sec:preprocessing}
This thesis aims to address the challenge of computing the JSR by combining two existing algorithms that have demonstrated practical effectiveness in calculationg the JSR for nontrivial sets of matrices. Both algorithms are based on the following simple concept:

We want to find the JSR of the finite set of matrices $\mathcal{A} = \{A_1, \cdots, A_n\}$
\begin{enumerate}
    \item \textbf{Assumptions}: $\mathcal{A}$ is irreducible and posesses the finiteness property. 
    \item \textbf{Candidates}: Efficiently find products $P = A_{i_k} \cdots A_{i_1}$ of matrices from $\mathcal{A}$ that maximize the averaged-spectral radius $\hat{\rho} := \rho(P)^\frac{1}{k}$ for a given maximal length $k_{\text{max}}$.
    \item \textbf{Rescaling}: Transform $\mathcal{A} \to \tilde{\mathcal{A}}$ with $\tilde{A_i} := \frac{1}{\hat{\rho}} A_i$.
    \item \textbf{Proofing}: Now establish the fact that JSR$(\tilde{\mathcal{A}}) = 1$ using the three-member-inequality. By homogenity this is equivalent to JSR$(\mathcal{A}) = \hat{\rho}$.
\end{enumerate}

The considered algorithms only differ in step 4, while the invariant-polytope algorithm tries to find a norm that bounds the products of length 1 already enough. The finite tree algorithm, on the other hand, bounds the products using some partitioning-space that separates every product into products that are 1-bounded and some rest-term that doesnt grow fast enough to overcome the k-th root of the JSR definition (polynomial growth).
By integrating these algorithms into a hybrid approach, this work aims to advance the computational tools available for JSR analysis combining efficiency and a vast space of matrix sets for which the algorithm terminates.
