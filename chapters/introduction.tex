
\chapter{Introduction}
\label{ch:introduction} % fÃ¼r einen Textverweis auf das Kapitel: \ref{ch:introduction}

The \textit{Joint Spectral Radius} (JSR) was first introduced by G.-C. Rota and G. Strang in 1960 \citep{rotaNoteJointSpectral1960}. They described the JSR as the maximal exponential growth rate of a product of matrices taken from a finite set. Since its inception, the JSR has become a cornerstone in various mathematical and engineering disciplines due to its ability to encapsulate the asymptotic behavior of matrix long products. 

The concept gained significant traction in the 1990s when researchers began exploring its theoretical properties and practical implications. Notable advancements include its application in wavelet theory, where it assists in the construction of refinable functions \citep{daubechies1992sets} as well as in control theory, where it is used to analyze the stability of switched linear systems \citep{blondelSurveyComputationalComplexity2000}, for which we will give a small example in the following. The computational challenges associated with determining the JSR have inspired the development of several algorithms, such as the invariant-polytope method \citep{guglielmiExactComputationJoint2013} and the finite-tree method \citep{mollerTreebasedApproachJoint2014}.

Despite the progress, the JSR computation remains a challenging problem, particularly due to the exponential complexity of exploring all possible matrix products. This thesis seeks to contribute to this ongoing effort by leveraging the invariant-polytope algorithm and the finite-tree algorithm to create a hybrid methodology that mitigates their respective limitations.

\section*{Structure of the Thesis}
The remainder of this thesis is structured as follows: Chapter~\ref{ch:introduction} provides a sufficient background on the JSR and its basic properties. Chapters~\ref{ch:inv.poly} and~\ref{ch:finite-tree} present the ideas and concepts of the algorithms that will be exploited to create the proposed hybrid approach, outlining their theoretical foundation and algorithmic implementation. Chapter~\ref{ch:hybrid} discusses possible combinations of former approaches, proposes the so-called Tree-flavored-invariant-polytope algorithm, and brings proofs of termination which is the main result of this thesis. Chapter~\ref{ch:numerics} presents numerical results to analyze the efficiency and applicability of the hybrid algorithm. Chapter~\ref{ch:conclusion} concludes with insights and future directions.

\section{Preliminaries}

In this thesis, the spectral radius of a matrix $A$ is denoted by $\rho(A)$. Unless stated otherwise, all matrices considered are assumed to have real entries, and all matrix norms are presumed to be submultiplicative. The analogous results for complex matrices follow similarly and may be reduced to the real case when necessary.

Let $\mathcal{A} = \{A_1, \dots, A_m\}$ denote a finite collection of matrices, each of size $d \times d$.

For any positive integer $k \in \mathbb{N}$, we define the set of positive multi-indices of length $k$ as

$$
    \mathcal{I}_k := \{1, \dots, m\}^k.
$$

Given a positive multi-index $I = [i_1, \dots, i_k] \in \mathcal{I}_k$, we define the associated matrix product by

$$
    A_I = A_{[i_1, \dots, i_k]} := A_{i_k} \cdots A_{i_1}.
$$

The set of all possible products of length $k$ formed from elements of $\mathcal{A}$ is then given by

$$
    \mathcal{A}^k := \left\{ A_I \mid I \in \mathcal{I}_k \right\}.
$$

For any scalar $\alpha \in \mathbb{R}$, we define the scaled matrix set by

$$
    \alpha \mathcal{A} := \left\{ \alpha A_1, \dots, \alpha A_m \right\}.
$$

\section{Motivation of the JSR}

Let $\mathcal{A} = \{A_1, \cdots, A_m\}$ be a finite set of $d\times d$ matrices. 
Consider a discrete-time linear switched system of the form:
\begin{equation} \label{eq:switched_system}
x_{k+1} = A_{\sigma(k)} x_k, \quad x_0 \in \mathbb{R}^d,
\end{equation}
where $\sigma: \mathbb{N} \to \{1, \dots, m\}$ is a switching signal.

A key question is whether the system is \emph{uniformly asymptotically stable}, i.e., whether every trajectory $(x_k)$ converges to zero for all initial states and switching sequences:
\[
\lim_{k \to \infty} x_k = 0.
\]
This behavior is governed by a generalization of the spectral radius for a single matrix the so-called \emph{joint spectral radius (JSR)} of a matrix set $\mathcal{A}$. \textcolor{red}{reference}
\begin{definition}
    \textcolor{red}{reference}
    For a finite set of matrices $\mathcal{A} = \{A_1, \dots, A_m\}$, the JSR is defined as:
    \begin{equation} \label{eq:jsr_def}
        \JSR(\mathcal{A}) := \lim_{k \to \infty} \max_{P \in \mathcal{A}^k}  \|P\|^{1/k}.
    \end{equation}
    where $\|\cdot\|$ denotes any matrix norm.
\end{definition}
This formulation captures the maximal asymptotic growth rate of matrix products.

\subsection*{Example}

Consider the matrix set:
\[
\mathcal{A} = \left\{ A_1 = 
\begin{bmatrix}
0.5 & 1 \\
0 & 0.5
\end{bmatrix}, \quad
A_2 =
\begin{bmatrix}
0.3 & 0 \\
1 & 0.3
\end{bmatrix}
\right\}.
\]
Each matrix individually has spectral radius less than $1$. However, their product
\[
A_1 A_2 =
\begin{bmatrix}
0.5 & 1 \\
0 & 0.5
\end{bmatrix}
\begin{bmatrix}
0.3 & 0 \\
1 & 0.3
\end{bmatrix}
=
\begin{bmatrix}
1.3 & 0.3 \\
0.15 & 0.15
\end{bmatrix}
\]
has spectral radius of approximately
\[
\rho(A_1 A_2) \approx 1.2929 > 1.
\]
This example shows that switching can amplify the effects of individual matrices, leading to divergent behavior even if all single matrices are contractive. Thus, the JSR can exceed the spectral radius of any individual matrix in the set.

The following result motivates the importance of bounding or approximating $\JSR(\mathcal{A})$. It is known that:
\begin{samepage}
    \begin{itemize}
        \item The system~\eqref{eq:switched_system} is \emph{uniformly bounded} (i.e., $\sup_k \|x_k\| < \infty$ for all $x_0 \in \R^d$) if and only if $\JSR(\mathcal{A}) \leq 1$.
        \item It is \emph{uniformly asymptotically stable} if and only if $\JSR(\mathcal{A}) < 1$. 
    \end{itemize}
    \citep{blondelSurveyComputationalComplexity2000}
\end{samepage}

Understanding the JSR is therefore critical for analyzing the asymptotic behavior of switched systems and motivates the development of reliable algorithms for finding or approximating the JSR, such as the hybrid algorithm proposed in \citep{mejstrikHybridApproachJoint2024}.
\textcolor{red}{also other important results depending on JSR}

\section{Theoretical background}

\begin{theorem}
    The JSR is well-defined and independent of the choice of the matrix norm.
\end{theorem}

\begin{proof}
Let $\| \cdot \|_1$   and $ \| \cdot \|_2 $ be two matrix norms on $ \mathbb{R}^{n \times n} $. By equivalence of norms in finite-dimensional vector spaces, there exist constants $ c, C > 0 $ such that:
$$
c \|P\|_1 \leq \|P\|_2 \leq C \|P\|_1 \quad \forall P \in \mathbb{R}^{d \times d}
.$$
Therfore we have that

\begin{align*}
& \lim_{k \to \infty} \max_{P \in \mathcal{A}^k} \|P\|_{1}^{1/k} 
\le  \lim_{k \to \infty} {\left(\frac{1}{c}\right)}^{\frac{1}{k}} \max_{P \in \mathcal{A}^k} \|P\|_{2}^{1/k} \\
= & \lim_{k \to \infty} \max_{P \in \mathcal{A}^k} \|P\|_{2}^{1/k} 
\le  \lim_{k \to \infty} {C}^{\frac{1}{k}} \max_{P \in \mathcal{A}^k} \|P\|_{1}^{1/k}\\
= & \lim_{k \to \infty} \max_{P \in \mathcal{A}^k} \|P\|_{1}^{1/k},
\end{align*}
which makes the limit independent of the choice of the used matrix norm. 
The existence of this limit is a special result from Fekete's Lemma which can bee seen in the appendix. \textcolor{red}{appendix ref} 
\end{proof}

To facilitate the analysis of the algorithms and presentation of the main results, we first establish fundamental properties of the JSR.

\subsection*{Homogeneity}
\begin{proposition}
    The JSR is homogeneous, meaning for any set of matrices $\mathcal{A}$ and scalar $\alpha \in \R$ we have
    \begin{equation}
        \JSR(\alpha \mathcal{A}) = |\alpha| \JSR(\mathcal{A}).
    \end{equation}
\end{proposition}
\begin{proof}
    Let $\mathcal{A} = \{A_1, \dots, A_m\}$ and $\alpha \in \mathbb{R}$. Then:
    \begin{align*}
        \JSR(\alpha \mathcal{A}) & = \lim_{k \to \infty} \max_{A_{i} \in \mathcal{A}^k} \|\alpha A_{i_k} \cdots \alpha A_{i_1}\|^{1/k} \\
        & = |\alpha| \lim_{k \to \infty} \max_{A_{i} \in \mathcal{A}^k} \|A_{i_k} \cdots A_{i_1}\|^{1/k} \\
        & = |\alpha| JSR(\mathcal{A}) \\
    \end{align*}
\end{proof}
\textcolor{red}{qedhere}

\subsection*{Three-member inequality}
\textcolor{red}{reference}
\begin{proposition}
    The $\JSR$ can be equally defined as
    $$
    \JSR(\mathcal{A}) = \limsup_{k \to \infty} \max_{P \in \mathcal{A}^k} \rho(P)^{1/k},
    $$
    if $\mathcal{A}$ is a finite (or compact) set.
\end{proposition}

\textcolor{red}{proof here}

\begin{proposition}
    Let $\|\cdot\|$ be a matrix norm. The inequality
    \begin{equation}
    \max_{P \in \mathcal{A}^k} \rho(P)^{\frac{1}{k}} \leq \JSR(\mathcal{A}) \leq \max_{P \in \mathcal{A}^k} \| P\|^{\frac{1}{k}}
    \label{eq:three-member}
    \end{equation}
    holds for every $k \in \mathbb{N}$.
\end{proposition}
\begin{proof}
    For a fixed $k \in \N$ let 
    $$
    P_{\text{max}} = \argmax_{P \in \mathcal{A}^k} \rho(P)^{\frac{1}{k}}.
    $$ 

    Then for the left-hand side we have

    \begin{align*}
        \max_{P \in \mathcal{A}^k} \rho(P)^{\frac{1}{k}} & = \rho(P_{\text{max}})^{\frac{1}{k}} \\
        & = \rho(P_{\text{max}})^{\frac{r}{kr}} \quad \forall r \in \N \\
        & = \rho(P_{\text{max}}^{r})^{\frac{1}{kr}} \quad \forall r \in \N\\
        & \leq \max_{P \in \mathcal{A}^{kr}} \rho(P)^{\frac{1}{kr}} \quad \forall r \in \N\\
        & \leq \limsup_{r \to \infty} \max_{P \in \mathcal{A}^{kr}} \rho(P)^{1/kr}\\
        & \leq \limsup_{r \to \infty} \max_{P \in \mathcal{A}^{r}} \rho(P)^{1/r}\\
        & = \JSR(\mathcal{A}).
    \end{align*}

    The right-hand side of the equation follows from a special case of Fekete's lemma which can be seen in the appendix. 
    \textcolor{red}{appendix ref No. [DL1992]} 
\end{proof}
This result forms a starting point for many computational approaches as the bounds are sharp in the sense that both sides converge to the JSR as $k\rightarrow \infty$ (left side in $\limsup$).
\textcolor{red}{left side is not generally converging but but its lim sup}

\subsection*{Similarity and reducibility}

\begin{proposition}
    The $\JSR$ is invariant under similarity transformations i.e. for any invertable matrix $T$ we have
    \begin{equation}
       \JSR(\mathcal{A}) = \JSR(T^{-1}\mathcal{A}T).
    \end{equation}
\end{proposition}

\begin{proof}
    This follows from the definition of the JSR and the properties of the spectral radius.
    \begin{align*}
    \JSR(T^{-1}\mathcal{A}T) & = \limsup_{k \to \infty} \max_{A_i \in \mathcal{A}^k} \rho(T^{-1}A_{i_k}T \cdot ... \cdot T^{-1}A_{i_1}T)^{1/k}\\
    & = \limsup_{k \to \infty} \max_{A_i \in \mathcal{A}^k} \rho(T^{-1}A_{i_k} \cdot ... \cdot A_{i_1}T)^{1/k}\\
    & = \limsup_{k \to \infty} \max_{A_i \in \mathcal{A}^k} \rho(A_{i_k} \cdot ... \cdot A_{i_1})^{1/k}\\
    & = \JSR(\mathcal{A})   
    \end{align*}
\end{proof}

\begin{definition}
    A set of matrices is called \emph{(commonly) reducible} if there exists a nontrivial subspace of $\mathbb{R}^n$ that is invariant under all matrices in the set. This means there exists a change of basis that block-triangularizes all matrices in $\mathcal{A}$ at the same time. If $\mathcal{A}$ is not reducible it is called \emph{irreducible}. 
\end{definition}

\begin{proposition}
    \label{prop:triangularized}
    If $\mathcal{A}$ is reducible, then 
    \begin{equation}
        \JSR(\mathcal{A}) = \max \{ \JSR(\mathcal{B}), \JSR(\mathcal{D}) \},
    \end{equation}
    where $\mathcal{B} = \{B_1,\dots, B_m\}$ and $\mathcal{D} = \{D_1,\dots, D_m\}$ are the blocks of the block-triangularized matrices
    \[
        T^{-1}A_jT = 
        \begin{bmatrix}
        B_j & C_j \\
        0   & D_j 
        \end{bmatrix}
    \]
    under some invertable matrix $T$. 
\end{proposition}
\textcolor{red}{proof here}
\begin{example}
    \[
    A_1 = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}, \quad
    A_2 = \begin{bmatrix} 3 & 2 \\ 2 & 3 \end{bmatrix}
    \]

    These matrices are reducible because there exists a nontrivial change of basis that transforms them into block-triangular form. The eigenvectors common to both matrices are

    \[
    v_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}, \quad
    v_2 = \begin{bmatrix} 1 \\ -1 \end{bmatrix}.
    \]

    Using the change of basis matrix

    \[
    T = \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix},
    \]

    we compute the similarity transformations

    \[
    T^{-1} A_1 T = \begin{bmatrix} 3 & 0 \\ 0 & 1 \end{bmatrix}, \quad
    T^{-1} A_2 T = \begin{bmatrix} 5 & 0 \\ 0 & 1 \end{bmatrix}.
    \]

    Since both matrices are upper triangular in this basis, they share a common invariant subspace, proving that they are reducible.
    Now from proposition~\ref{prop:triangularized} we know that the $\JSR$ can be calculated with 
    \[ 
    \JSR(\mathcal{A}) = \max \{ \JSR(\{3,5\}), \JSR(\{1,1\})\} = 5
    \]
\end{example}

This can be applied iteratively until the sets of blocks are all irreducible.
The problem was split into similar problems of smaller dimension.
For the following considerations we can now assume $\mathcal{A}$ to be irreducible.

\subsection*{Infimum over norms}
\begin{proposition}
    \textcolor{red}{reference Rota und Strang 1960}
    The JSR can be equivalently defined as the infimum over all submultiplicative norms:
    \begin{equation}
        \JSR(\mathcal{A}) = \inf_{\|\cdot\|} \max_{A \in \mathcal{A}} \|A\|.
    \end{equation}
\end{proposition}

\begin{proof}
    This is done by defining the family of norms $\|x\|_{\epsilon} := \max \{ \| \frac{A}{\lambda + \epsilon}x\|_2 : A \in \mathcal{A}\}$,
    where $\epsilon > 0$ and $\lambda$ is the $\JSR$ of the set $\mathcal{A}$.
    Now, using the induced matrix norm we arrive at: 
    $$ \sup _{A \in \mathcal{A}} \|Ax\|_{\epsilon} \leq \lambda + \epsilon$$

\textcolor{red}{qedhere and if $\mathcal{A} is irreducible than the infimum is attained [BergerWang]$}

\end{proof}

\textcolor{red}{might not be needed so may be disregarded in the future}

\subsection*{Finiteness property}
\begin{definition}
    A matrix set $\mathcal{A} = \{ A_1, \dots, A_m \}$ is said to possess the \emph{finiteness property} if there exists a finite sequence of matrices $A_{i_1}, \dots, A_{i_k}$ such that
    \begin{equation}
        \operatorname{JSR}(\mathcal{A}) = \|A_{i_k} \cdots A_{i_1}\|^{1/k}.
    \end{equation}
    Any such product is referred to as a \emph{spectrum-maximizing product} (s.m.p.).
    \textcolor{red}{[Begarias, Wang 1995]}
\end{definition}
Although the finiteness property does not hold for all matrix sets, it is crucial for the effectiveness and termination of most algorithmic approaches.
\textcolor{red}{[Bousch, Mairesse 2002]}

\subsection*{Complexity}

The computation of the joint spectral radius (JSR) is theoretically challenging due to several complexity results.

Determining whether the JSR of a given set of matrices is below a threshold is \textbf{NP-hard} \citep{tsitsiklis1997lyapunov}, meaning no polynomial-time algorithm is expected unless $\text{P} = \text{NP}$. Moreover, for general sets of matrices, even deciding whether the JSR is strictly less than one is \textbf{undecidable} \citep{blondel2000boundedness}. Despite these theoretical limitations, many practical cases allow for efficient numerical estimation.

\textcolor{red}{Unlike the spectral radius of a single matrix, the JSR is often \textbf{non-algebraic} \citep{guglielmiExactComputationJoint2011}, meaning it cannot always be expressed as a root of a polynomial with rational coefficients.(needs refactoring from result of [Kozyakin])}

These complexity results highlight fundamental challenges. There are still applied settings where efficient approximation algorithms provide useful results.

\subsection*{Candidates and generators}
Approximating the JSR, using the invariant-polytope or the finite-tree algorithms, requires identifying candidate products or \emph{generators} of the matrix set that contribute most significantly to the asymptotic growth rate. These generators are often derived through optimization techniques and their identification is a key step in computational algorithms.

\subsection*{Spectral gap}
\begin{definition}
\textcolor{red}{include}
\end{definition}

\begin{remark}
    \textcolor{red}{It could be hybrid doesnt need spectral gap if JSR was guessed correctly upfront}
\end{remark}

\begin{example}
    \textcolor{red}{include}
\end{example}

\section{Preprocessing}
\label{sec:preprocessing}
This thesis aims to address the challenge of computing the JSR by combining two existing algorithms that have demonstrated practical effectiveness in calculationg the JSR for nontrivial sets of matrices. Both algorithms are based on the following simple concept:

We want to find the JSR of the finite set of matrices $\mathcal{A} = \{A_1, \cdots, A_n\}$
\begin{enumerate}
    \item \textbf{Assumptions}: $\mathcal{A}$ is irreducible. 
    \item \textbf{Candidates}: Efficiently find products $P = A_{i_k} \cdots A_{i_1}$ of matrices from $\mathcal{A}$ that maximize the averaged-spectral radius $\hat{\rho} := \rho(P)^\frac{1}{k}$.
    \item \textbf{Rescaling}: Transform $\mathcal{A} \to \tilde{\mathcal{A}}$ with $\tilde{A_i} := \frac{1}{\hat{\rho}} A_i$.
    \item \textbf{Proofing}: Now establish the fact that JSR$(\tilde{\mathcal{A}}) = 1$ using the three-member-inequality. By homogenity this is equivalent to JSR$(\mathcal{A}) = \hat{\rho}$.
\end{enumerate}

The considered algorithms only differ in step 4, while the invariant-polytope algorithm tries to find a vector norm whos induced matrix norm bounds the matrices from $\mathcal{A}$ already enough. The finite-tree algorithm, on the other hand, bounds the products using some partitioning-space that separates every product into products that are bounded by 1 and some rest-term that does not grow fast enough to overcome the k-th root of the JSR definition (polynomial growth).
By integrating these algorithms into a hybrid approach, this work advances the computational tools available for JSR analysis combining efficiency and a vast space of matrix sets for which the algorithm terminates.
